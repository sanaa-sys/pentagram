{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkgc6UZAl7y77XmNWAt4/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaa-sys/pentagram/blob/main/Image_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from fastapi import FastAPI, HTTPException, Query\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "import uvicorn\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "# Define FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Hugging Face API details\n",
        "HF_API_URL = \"https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5\"\n",
        "HF_API_TOKEN =  userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# Headers for Hugging Face API\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {HF_API_TOKEN}\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "O8Iksdseto-o"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageRequest(BaseModel):\n",
        "    text: str\n",
        "    userId: str\n",
        "\n",
        "@app.post(\"/api/generate-image\")\n",
        "async def generate_image(request_data: ImageRequest): # Added Query\n",
        "    max_retries = 3  # Try up to 3 times\n",
        "    retry_delay = 60  # Wait 60 seconds between\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Validate input\n",
        "            if not request_data.text or not request_data.userId: # Changed request.text to text, request.userId to userId\n",
        "                raise HTTPException(status_code=400, detail=\"Text and userId are required.\")\n",
        "            # Send request to Hugging Face API\n",
        "            response = requests.post(\n",
        "                HF_API_URL,\n",
        "                headers=headers,\n",
        "                json={\"inputs\": request_data.text} # Changed params to json to work with post and used text instead of passing all the data.\n",
        "            )\n",
        "            # Check for errors in the response\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(status_code=response.status_code, detail=response.text)\n",
        "\n",
        "            # Convert the binary image response to a base64 string\n",
        "            image_data = response.content\n",
        "            img_base64 = base64.b64encode(image_data).decode(\"utf-8\")\n",
        "\n",
        "            if response.status_code == 503:\n",
        "                try:\n",
        "                    estimated_time = response.json()[\"estimated_time\"]\n",
        "                    print(f\"Model is loading, estimated time: {estimated_time} seconds. Attempt {attempt + 1}/{max_retries}\")\n",
        "                    time.sleep(estimated_time + 5)\n",
        "                except:\n",
        "                    print(f\"Model is loading, can't calculate time. Attempt {attempt + 1}/{max_retries}\")\n",
        "                    time.sleep(retry_delay)\n",
        "            else:\n",
        "                raise HTTPException(status_code=response.status_code, detail=response.text)\n",
        "\n",
        "            # Return the base64 string to the frontend\n",
        "            return {\n",
        "                \"message\": \"Image generated successfully.\",\n",
        "                \"imageData\": img_base64,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during request: {e}. Attempt {attempt + 1}/{max_retries}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)  # Wait before the next retry\n",
        "            else:\n",
        "                raise HTTPException(status_code=500, detail=f\"Failed after multiple retries: {e}\")\n",
        "\n",
        "async def run_server():\n",
        "    # This is now an async function that can be awaited\n",
        "    config = uvicorn.Config(app=app, host=\"0.0.0.0\", port=8000)\n",
        "    server = uvicorn.Server(config=config)\n",
        "    await server.serve()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Wrap the async function call\n",
        "    async def main():\n",
        "        await run_server()\n",
        "\n",
        "    # Set the authtoken\n",
        "    ngrok.set_auth_token(userdata.get(\"ngrok_token\")) # Replace with your ngrok authtoken\n",
        "    # Open a tunnel to port 8000\n",
        "    tunnel = ngrok.connect(8000)\n",
        "    print(\"Public URL:\", tunnel.public_url)\n",
        "\n",
        "    # Start the server in a separate thread.\n",
        "    # Changed: Instead of calling a normal function, use loop.run_until_complete() to run the async function\n",
        "    print(\"FastAPI server started in a separate thread.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y870PSfEwAXB",
        "outputId": "f4d76a9f-4b40-44f3-8d53-17af5e78d5d3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://d306-34-48-187-203.ngrok-free.app\n",
            "FastAPI server started in a separate thread.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "bL9_pDwRBTA4"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}